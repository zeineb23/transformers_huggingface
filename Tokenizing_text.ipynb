{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "\n"
      ],
      "metadata": {
        "id": "aoFJg3F47MWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we collect Data, we get raw text. Meanwhile, the model is unable to read text, it processes numbers. Therefore we need to tokenize texts to make them understandable to the model."
      ],
      "metadata": {
        "id": "QGLulO_p9SV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"My name is Zeineb, I am 23 years old and I want to learn more about model training and fine tuning.\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHhxsj9L7Riy",
        "outputId": "2c994270-1e26-464b-bd2a-665254c86847"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['My', 'name', 'is', 'Z', '##ein', '##eb', ',', 'I', 'am', '23', 'years', 'old', 'and', 'I', 'want', 'to', 'learn', 'more', 'about', 'model', 'training', 'and', 'fine', 'tuning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert to IDs to be read by the model**"
      ],
      "metadata": {
        "id": "09K73dt57Xlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sidyCtdP7hSC",
        "outputId": "91a93a33-df79-45b9-fd95-79af6a3f414b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1422, 1271, 1110, 163, 20309, 15581, 117, 146, 1821, 1695, 1201, 1385, 1105, 146, 1328, 1106, 3858, 1167, 1164, 2235, 2013, 1105, 2503, 19689, 119]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoding Ids to get text**"
      ],
      "metadata": {
        "id": "oiZl1g0181EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_string = tokenizer.decode(ids)\n",
        "print(decoded_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8hIw7PB83O8",
        "outputId": "01100f4f-3f9e-4fed-a47b-24abb58f835e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Zeineb, I am 23 years old and I want to learn more about model training and fine tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling multiple sentences**"
      ],
      "metadata": {
        "id": "ySKUE2rX_XTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n"
      ],
      "metadata": {
        "id": "DgDoapr-83xG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will tokenize a list of sentences :"
      ],
      "metadata": {
        "id": "LXCtpeGUAj6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences =[\"I am currently a software engineering student.\", \"I still want to learn so many things about data science and machine learning.\", \"Education never ends!\",]\n",
        "\n",
        "tokens = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
        "\n",
        "for id in ids :\n",
        "  print(\"ID : \", id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF7Au7EO_1Pt",
        "outputId": "74f81d1d-d774-43ea-ddfa-4d04a8837462"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID :  [1045, 2572, 2747, 1037, 4007, 3330, 3076, 1012]\n",
            "ID :  [1045, 2145, 2215, 2000, 4553, 2061, 2116, 2477, 2055, 2951, 2671, 1998, 3698, 4083, 1012]\n",
            "ID :  [2495, 2196, 4515, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to convert them into a tensor so that the model can read them. The prolem is : the tensor needs to have a list of arrays with the same length, that's why we willa dd the attribute padding=True (it adds a list of 0 and a mask)"
      ],
      "metadata": {
        "id": "kH0zvEqOAosN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "ids = tokenizer(sentences, padding = True)\n",
        "all_ids=torch.tensor(ids.input_ids)\n",
        "\n",
        "#prediction\n",
        "output=model(all_ids)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yO3yIVw9Ce7F",
        "outputId": "d412059d-ab37-44e0-f790-84ee2537998d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.4805, -0.2369],\n",
            "        [-2.8513,  2.9278],\n",
            "        [-0.5036,  0.5805]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd9kA2ekFjrN",
        "outputId": "ebfc4a0e-00e3-4f2e-ce72-f296e44dcf09"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4805, -0.2369],\n",
            "        [-2.8513,  2.9278],\n",
            "        [-0.5036,  0.5805]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**"
      ],
      "metadata": {
        "id": "T66t1fcPG7wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrWGpXevG95V",
        "outputId": "fe0b8661-d2da-4252-867d-244b19db9b9a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
            "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OumXIzN_G-U2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}